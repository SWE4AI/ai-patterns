const e="062",t="Prediction cache pattern",a="",c=`When your service may get prediction request on same data, and can identify it.
The prediction result may not change so frequently.
The input data can be searched with cache.
To accelerate prediction and offload.`,i="The prediction server or proxy will store input data as cache key with prediction as value, if the key does not exist. After the cache, the cache search and prediction will be executed parallelly and returns the value if the cache hits without waiting for the prediction completes.",o=`It will shorten the amount of time taken to predict with less load to the prediction server.
The amount of data to cache may need to be considered with balance of cost and volume. The unit price for cache space tends to be higher with less size than storage or database, hence it is recommended to plan a policy to clear cache.
If the prediction result changes with time, it requires to clear old cache to prevent responding with outdated prediction. If the service gets high load that the cache size increases rapidly, it is important to plan cache clear policy concretely.`,n="https://github.com/shibuiwilliam/ml-system-in-actions/tree/main/chapter4_serving_patterns/prediction_cache_pattern",s=["Data Cache Pattern"],r=["Architecture"],h=["124"],d={id:e,name:t,aka:a,motivation:c,solution:i,consequences:o,examples:n,related:s,categories:r,resources:h};export{a as aka,r as categories,o as consequences,d as default,n as examples,e as id,c as motivation,t as name,s as related,h as resources,i as solution};
