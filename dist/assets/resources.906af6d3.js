const e=[{id:"001",name:"Encapsulating ML Models within Rule-based Safeguards",aka:"",motivation:"It is impossible to guarantee the correctness of ML model predictions, so they should not be directly used for safety- or security-related functions. Furthermore, ML models can be unstable and vulnerable to adversarial attacks, data noise, and drift.",solution:"Introduce a deterministic, rule-based mechanism that decides what to do with the prediction results, e.g., based on additional quality checks.",consequences:"Reduced risk for negative impacts of incorrect predictions, but a more complex architecture.",examples:"",related:"",categories:"Security & Safety",resources:"001, 012, 012112, 012113, 012128"},{id:"002",name:"Pipes and Filters",aka:"",motivation:"A system has to perform a variety of tasks of varying complexity on the data that it processes.",solution:"Data is processed through a series of procedures, which have different inputs at various stages and produce incremental results as the comparison of the query images and input image takes place.",consequences:"System becomes more flexible; removing, modifying and adding filters is easier than with a monolithic system",examples:"",related:"",categories:"Architecture, Traditional",resources:"002"},{id:"003",name:"Client-Server",aka:"",motivation:"A number of users/applications requires data from an application.",solution:"One component has a role of server and at least one component has the role of client, initiating connections in order to obtain some service.",consequences:"The data, as well as the network peripherals, are centrally controlled. A disadvantage, however, is that the server is expensive to purchase and manage.",examples:"A classic example of this architecture pattern is the World Wide Web.",related:"",categories:"Architecture, Traditional",resources:"002"},{id:"004",name:"Model-View-Controller (MVC)",aka:"",motivation:"The system is required to facilitate multiple views of the same data object and single view of multiple data objects.",solution:"Includes three parts: Model (internal data), View (data representation) and Controller (I/O Controller). Only the model layer should be altered.",consequences:"This pattern greatly enhances the code reusability and system enpansibility.",examples:"in Reference 026",related:"",categories:"Architecture, Traditional",resources:"002, 026, 012032"},{id:"005",name:"Multi-Layer Pattern",aka:"Separation of Concerns, Multi-Tiered Architecture",motivation:"An application comprises several groups of subtasks, each of which is at a different level of abstraction.",solution:"Divide the application into different layers. Each layer consists of submodules, and can be independently designed to feed into foreign prototypes. It has an input and corresponding output to the succeeding layer. The succeeding return is used as a feed to the next layers for further processing. Every layer communicates only with its direct neighbor.",consequences:"This pattern enables inference of results at the individual steps. Extra components can be added or modified to meet the computational requirements, giving it high flexibility.",examples:"024, 022, 020",related:"",categories:"Architecture, Traditional",resources:"002, 020, 022, 024, 001012, 005009, 010003, 012005, 012017, 012032, 012112, 012134"},{id:"006",name:"Player-Role Pattern",aka:"",motivation:"The training set, data pre-processing and its training play many roles when attached to other detection systems or systems that widely use machine learning techniques.",solution:"Players can change roles given different sets of requirements and scenarios.",consequences:"",examples:"",related:"",categories:"Implementation, Traditional",resources:"002"},{id:"007",name:"Ethics credentials",aka:"Verifiable ethical credentials",motivation:"Responsible AI requirements are either omitted or mostly stated as high-level objectives, and not specified explicitly in a verifiable way as expected system outputs. Because of this, users may trust an AI system less, or even refrain from using it.",solution:"Provide verifiable ethics credentials for your AI system or component. Using publicly accessible and trusted data infrastructure, the credentials can be verified as proof of ethical compliance. Additionally, users may also have to verify their credentials before getting access to the AI system.",consequences:"Trust and system acceptance increases, and awareness of ethical issues is raised. However, a trusted public data infrastructure is needed, and credentials need to be maintained and potentially refreshed from time to time.",examples:"",related:"",categories:"Security & Safety",resources:"003, 129"},{id:"008",name:"AI Mode Switcher",aka:"",motivation:"AI is often complex and hard to explain, thus making detailed risk assessment difficult. Adopting AI can be considered as a major architectural design decision when designing a system.",solution:"An AI component is designed to be flexibly switched off at run-time.",consequences:"",examples:"",related:"Decision Mode Switcher",categories:"Security & Safety",resources:"003"},{id:"009",name:"Decision Mode Switcher",aka:"",motivation:"AI is often complex and hard to explain, thus making detailed risk assessment difficult. Adopting AI can be considered as a major architectural design decision when designing a system.",solution:"Provision various decision-making modes, i.e. fully automatic or suggestion with human-in-the-loop (kill switch, override, fallback)",consequences:"",examples:"",related:"AI Mode Switcher",categories:"Security & Safety",resources:"003"},{id:"010",name:"Audit Black Box",aka:"",motivation:`There is a strong desire for continuously monitoring and validating AI systems post deployment for ethical requirements but current operation practices provide lim-
ited guidance.`,solution:`Ethical metric data is captured at run time
by a black box for auditing purposes.`,consequences:"An AI system can be monitored continuously, but auditing introduces more complexity and effort. Designing the Black Box is challenging as the ethical metrics need to be identified for data collection.",examples:"",related:"",categories:"Security & Safety",resources:"003,129"},{id:"011",name:"AI Pipelines",aka:"Sequential Decomposition",motivation:"Complex prediction or synthesis use cases are often difficult to accomplish with a single AI tool or model.",solution:"Divide the problem into smaller consecutive steps, then combine several existing AI tools or custom models into an inference-time AI pipeline, where each specialized tool or model is responsible for a single step.",consequences:"More tools and models need to be integrated, but the provided is result is of higher quality. Each step can be optimized individually.",examples:"Typical computer vision inference pipelines as described in 126",related:"Pipes and Filters",categories:"Architecture, Topology",resources:"008, 126, 103"},{id:"012",name:"Secure Virtual Premise (SVP)",aka:"",motivation:`Machine Learning (ML) requires large training data sets which typically exist in large enterprises that can afford to develop ML concepts. Small companies, however, might
collect data or define models but must rely on collaborations with distributed stakeholders for developing AI solutions.
Hence, trusted collaborations in AI engineering are needed for empowering ML beyond large companies`,solution:`A SVP connects distributed computation and storage resources (both physical and virtual)
into a large virtual resource space for AI training and enforces a perimeter around this space.`,consequences:`Developers can focus on
application design while having access to AI artifacts and AI data, thus accelerating the system development.`,examples:"Presented in resource 008",related:"AI Pipelines",categories:"Security & Safety",resources:"008"},{id:"013",name:"Fluid Architecture",aka:"",motivation:"Classical architecture and UX patterns, though largely successful, suffer the following limitations in AI contexts: (1) Static interfaces, (2) Lack of bidirectionality, (3) Slow updates, (4) subiptimal personalization, (5) interpretability and explainability",solution:"Additional to any AI that runs in a backend, there is an AI in the frontend to evolve the interface and adapt it to changes. Moreover, there is an AI orchestrator. Its purposes are to monitor the performance of both the frontend\u2019s AI and the backend\u2019s AI to make sure that their respective performance is satisfactory and to witness contract negotiations to see how the interactions between the frontend and backend evolve over time.",consequences:"The aforementioned limitations are alleviated.",examples:"-",related:"",categories:"Architecture, Topology",resources:"010"},{id:"014",name:`Distinguish Business Logic from
ML Model`,aka:"",motivation:"Machine Learning (ML) systems are complex because their ML components must be (re)trained regularly and have an intrinsic non-deterministic behavior. Similar to other systems, the business requirements for these systems as well as ML algorithms change over time.",solution:`Define clear APIs between traditional and
ML components. Place business and ML components with different responsibilities into three layers. Divide data flows into three.`,consequences:"Decoupling \u201Ctraditional\u201D business and ML components allows the ML components to be monitored and adjusted to meet users\u2019 requirements and changing inputs.",examples:"",related:"Separation of Concerns, Multi-layer Pattern",categories:"Architecture",resources:"012, 001013, 012017, 012113, 012128"},{id:"015",name:"Delegation of Safety Responsibility",aka:"",motivation:"Recent developments in ML and DL allowed probabilistic systems with large input and output spaces to be explored in safety critical systems. This introduces safety-related challenges, such as: (1) Model complexity and opacity, (2) Dealing with probabilistic output, (3) Sensitivity to distribution shifts (4) Formal verification is impossible or not scalable and more. This safety responsibility has to be dealt with.",solution:"Delegate all safety responsibility to other components or wrap DL algorithms in safety envelopes.",consequences:"Increased Safety while putting limited constraints on the DL component. But increased complexity.",examples:"A computer vision based planning algorithm for autonomous vehicles might generate a series of unsafe manoeuvres or trajectories. However, safe executors will benefit from other sensors to delay execution of a maneouver or choose a different path in order to safely achieve the same goals",related:"Partial Rejection of Safety Responsibility, Full Acceptance of Safety Responsibility",categories:"Security & Safety",resources:"016"},{id:"016",name:"Partial Rejection of Safety Responsibility",aka:"",motivation:"Recent developments in ML and DL allowed probabilistic systems with large input and output spaces to be explored in safety critical systems. This introduces safety-related challenges, such as: (1) Model complexity and opacity, (2) Dealing with probabilistic output, (3) Sensitivity to distribution shifts (4) Formal verification is impossible or not scalable and more. This safety responsibility has to be dealt with.",solution:"A system decides to reject the predictions of a DL algorith or delay any decisions until it is confident enough",consequences:"One can allow some uncertainty in a system, while imposing relatively low constraints on the DL algorithms used.",examples:"A UAV operating at high altitudes can not encounter birds or similar objects. This property can easily be verified by checking the altitude (where altimeters corresponds to classic safety critical systems). If such a prediction is given by a sensing system, the system can decide to reject it or take further actions (such as to trigger a fail safe mode).",related:"Delegation of Safety Responsibility, Full Acceptance of Safety Responsibility",categories:"Security & Safety",resources:"016"},{id:"017",name:"Full Acceptance of Safety Responsibility",aka:"",motivation:"Recent developments in ML and DL allowed probabilistic systems with large input and output spaces to be explored in safety critical systems. This introduces safety-related challenges, such as: (1) Model complexity and opacity, (2) Dealing with probabilistic output, (3) Sensitivity to distribution shifts (4) Formal verification is impossible or not scalable and more. This safety responsibility has to be dealt with.",solution:"Verify certain safety properties to which the DL component will have to adhere.",consequences:"Removes uncertainty. However, a system implemented using this pattern will have to use less powerful algorithms. Very hard to implement for complicated tasks.",examples:`An intelligent planning algorithm
for a spacecraft embarked in deep space exploration, where
communication to Earth takes a very long time. Any nonverifiable properties of a component will most likely mark
it unusable and lead to new system designs.`,related:"Delegation of Safety Responsibility, Partial Rejection of Safety Responsibility",categories:"Security & Safety",resources:"016"},{id:"018",name:"Lambda Architecture Pattern",aka:"",motivation:"Different components in a system have different performance requirements that need to be satisfied, e.g., some are focused on throughput and others on response time.",solution:`Group the components based on their latency requirements into three layers:
(1) batch layer ingests and stores large amounts of data
(2) speed layer processes updates to the data in low-latency
(3) serving layer provides precalculated results in a low-latency fashion`,consequences:"",examples:"Software Architecture for Human-AI teaming in smart manifacturing",related:"",categories:"Architecture, Traditional",resources:"023, 005011, 012005, 012113, 012122, 012128"},{id:"019",name:"Deploy canary model",aka:"",motivation:"You trained a new model with assumed better prediction quality, but it's not certain if this will carry over to production. Additionally, there could be other quality issues with the new model that should not affect all users in production at once.",solution:"Deploy the new model in addition to the existing ones and route a small number of requests to it to evaluate its performance. If this test is successful, all existing models can be replaced. If not, the new model needs to be improved.",consequences:"Only a small number of users are subjected to potential bugs or low-quality predictions. Additional serving and monitoring infrastructure is required.",examples:"",related:"",categories:"Deployment, Traditional",resources:"012112, 012113, 012134"},{id:"020",name:"Stateless Serving Function",aka:"",motivation:"Serve thousands to millions of prediction requests per second.",solution:`Design ML system around stateless function that captures the architecture and weights of a trained model
1) Export the model into a format that captures the mathematical core of the model and is programming language agnostic
2) In the production system, the formula consisting of the "forward" calculations of the model is restored as a stateless function
3) The stateless function is deployed into a framework that provides a REST endpoint`,consequences:"",examples:"",related:"Batch Serving",categories:"Deployment, Traditional",resources:"101, 106"},{id:"021",name:"Continued Model Evaluation",aka:"",motivation:"Problem of needing to take action when a deployed model is no longer fit-for-purpose.",solution:"Continuously monitor the model's predictive performance with the same evaluation metrics used during development. This is used to determine if changes to a model are working as intended.",consequences:"",examples:`(1) Saving predictions
(2) Capturing ground truth`,related:"",categories:"Testing & Quality Assurance",resources:"101, 106, 012103"},{id:"022",name:"Batch Serving",aka:"",motivation:"Predictions need to be carried out asynchronously over large volumes of data (contrary to predictions for small, individual requests), e.g., generating personalized playlists every week. This is only applicable if there is no need for (near-)real-time predictions.",solution:"Use distributed data processing infrastructure (e.g, based on MapReduce) to asynchronously carry out complex ML inference tasks on a large number of computing nodes. The individual predictions are aggregated back into a single result.",consequences:"Positive: You can manage server resources flexibly and in strict accordance with demand. You may rerun the job in case of error. There is no requirement for high availability in your server system. Negative: You need a job management server. This pattern depends on the ability to split a task across multiple workers.",examples:"",related:"Stateless Serving Function",categories:"Deployment, Traditional",resources:"101, 106, 124"},{id:"023",name:"Workflow Pipeline",aka:"",motivation:"Creating an end-to-end reproducible training and deployment pipeline for a machine learning component is difficult. Data science notebooks can run a whole pipeline, but they do not scale.",solution:"Make each pipeline step a separate containerized service. Services are orchestrated and chained together to form pipelines that can be run via REST API calls.",consequences:"The portability, scalability, and maintainability of the individual pipeline steps is improved, at the cost of an overall more complex solution.",examples:"012024",related:"",categories:"Architecture, Topology",resources:"008, 009, 125, 126, 101, 102, 106, 112, 103, 012014, 012024"},{id:"024",name:"Proxy Pattern",aka:"",motivation:`A substitute for the production database or service is needed. It should: 
1) scale serving to multiple machines, but only provide one endpoint
2) reduce the needed amount of computational resources`,solution:`1) Use a reverse proxy to serve models
2) Use a cache proxy to serve queries`,consequences:"",examples:"",related:"",categories:"Architecture, Traditional",resources:103},{id:"025",name:"Mediator Pattern",aka:"",motivation:`AI-based services are provided
directly to downstream applications (e.g., e-commerce sites, trade orders), therefore coupling services and downstream applications tighter than needed.`,solution:`Define an object that encapsulates how a set of objects interact. Instead of providing services directly, each service has to go through the mediator. The mediator may also perform checks or update widgets (e.g., deduplicate items).
This restricts direct communication between services.`,consequences:"Loose coupling between sub-systems.",examples:"",related:"",categories:"Architecture, Traditional",resources:"103, 120"},{id:"026",name:"Continuous Integration and Deployment",aka:"",motivation:"There is a need to reduce the risk of releasing broken applications.",solution:"Always build with unit and component tests and deploy with verification tests using code that itself is under version control. After you commit to a development branch, the system deploys to a development environment. Once all end-to-end and manual smoke testing is complete, a manual action deploys to production.",consequences:"",examples:"012011",related:"Continued Model Evaluation",categories:"Process, Traditional",resources:"109, 012122, 012011"},{id:"027",name:"Infrastructure as Code",aka:"",motivation:"One wants to avoid deploying or configuring the infrastructure incorrectly",solution:"Use code to specify the infrastructure and run scripts to recreate and verify the infrastructure that the system needs. Similarly, the infrastructure required to build and test the ML models, as well as run them in production, should be defined as code.",consequences:"",examples:"",related:"Serving template pattern",categories:"Process, Traditional",resources:109},{id:"028",name:"End-To-End Tests",aka:"",motivation:"A faulty ML model or pipeline shouldn\u2019t be allowed to be released and produce poor results in the app.",solution:"Manual smoke testing is always useful, and keeping the tests fresh and up-to-date with new features, use cases and data is an ongoing task. ML model predictions are no different. If a part of the app is served by an ML model recommendation, identify the assertions that can be made",consequences:"",examples:"",related:"",categories:"Testing & Quality Assurance, Traditional",resources:109},{id:"029",name:"Alarms And Notifications On Processes",aka:"",motivation:`In an ML system, there\u2019s incoming data, models are run, the models\u2019 output is stored and analyzed and application tables are created. All these processes are either running on regularly scheduled jobs or a part of a queue or eventing system. 
Errors shall be detected`,solution:"Whenever a script fails, log the error and push it to an alarming dashboard (to debug later) and notify employees via email, Slack or some other method. When a notification is superfluous, adjust the alarm: Every alarm should be an occasion that requires action. Similar to latency in an application, store and analyze the results of an ML model application that will help data changes, infrastructure capacity pressures or just unexpected drifts of prediction types.",consequences:"",examples:"",related:"",categories:"Testing & Quality Assurance",resources:109},{id:"030",name:"Functional-Style Architecture",aka:"",motivation:"ML systems require a significant amount of data processing and transformation. The deployed application, however, can become very complex and difficult to manage if this pattern is replicated.",solution:"A functional-style approach of making discrete transformations to data and passing results to the next stage allows for processes to be better optimized and managed, reducing memory and storage requirements while also increasing efficiency. Additionally, as elastic cloud systems may lose nodes from time to time, queuing and messaging can buffer data coming into the ML system and ensure everything gets processed.",consequences:"",examples:"",related:"",categories:"Architecture, Traditional",resources:109},{id:"031",name:"Legacy",aka:"",motivation:"An older model or early approach needs to be replaced but has entrenched support",solution:`Use as an input to new approach (presumably based on ML) 
Can be technically challenging but frequently can be converted to an input in conjunction with Pipeline`,consequences:"",examples:"",related:"Chop Shop, Tiers, Shadow",categories:"Process, Traditional",resources:112},{id:"032",name:"Shadow",aka:"",motivation:"A legacy system is an input to critical processes and operations",solution:`Develop a new system and run it in parallel to test output or regularly audit.
Applies to upgrades to input pipeline.`,consequences:"",examples:"",related:"",categories:"Process, Traditional",resources:112},{id:"033",name:"Chop Shop",aka:"",motivation:`Legacy system represents significant investment of resources
often rule based and capture valuable domain features`,solution:"Isolate features and measure computing costs. Use selected features in new models and processes.",consequences:"",examples:"",related:"Legacy, Adversarial",categories:"Process, Traditional",resources:112},{id:"034",name:"Internal Feedback",aka:"",motivation:"A low risk way to test new models with live users is needed.",solution:`Use your own product internally.
Give internal users a way to turn on new models, use the product, and give feedback.
Note: this is also used to develop training data.`,consequences:"",examples:"",related:"Bathwater, Follow the Crowd",categories:"Process, Traditional",resources:112},{id:"035",name:'Handshake or "Hand Buzzer',aka:"",motivation:"System depends on inputs delivered outside of the normal release process.",solution:`Create a "handshake" normalization process
Release handshake process as software associated with input and version.
Regularly check for significant changes and send alerts.`,consequences:"",examples:"",related:"",categories:"Architecture, Traditional",resources:"112, 012112, 012134"},{id:"036",name:"Replay",aka:"",motivation:"Need a way to test models and operational data.",solution:"Invest in a batch test framework.",consequences:"",examples:`Web search: replay query logs and look at changes in rank of clicked documents
- Recommender systems
- Messaging inbox replay`,related:"",categories:"Testing & Quality Assurance, Traditional",resources:112},{id:"037",name:"Tar pit",aka:"",motivation:`The system needs to identify bad entities but cost to register new ones is cheap. This slows
down adversary's evolution.`,solution:"Don't reject, delete or notify bad actors.",consequences:"",examples:"Slow down email messaging for low-reputation IP adresses",related:"honey trap",categories:"Architecture, Traditional",resources:112},{id:"038",name:"Giveaway",aka:"",motivation:"Need low risk testing or new data.",solution:`Give away service to non-customers
give away a related service`,consequences:"",examples:"",related:"honey trap",categories:"Process, Traditional",resources:112},{id:"039",name:"Two-Phase Predictions",aka:"Cascade Predictions, Multiple stage prediction pattern",motivation:"Executing large, complex models can be time-consuming and costly, especially if lightweight clients like mobile or IoT devices are involved.",solution:"Split the prediction into two phases. A simple, fast model is executed first on the client. Afterwards, a large, complex model is optionally executed in the cloud for deeper insights.",consequences:"Prediction response time is reduced for some cases. The number of large, expensive predictions is reduced. The client has a fall-back model when there is no Internet connection.",examples:"Voice activation in AI assistants like Alexa or Google Assistant",related:"",categories:"Deployment",resources:"101, 106, 124"},{id:"040",name:"Model Embedded in Application",aka:"",motivation:"A simple(r) deployment model is needed.",solution:"The trained model is embedded in the application as a dependency.",consequences:`pre-trained: yes
on-the-fly-predictions: yes

To update the model, the whole application needs to be redeployed.`,examples:"",related:"",categories:"Deployment",resources:113},{id:"041",name:"Dedicated Model API",aka:"",motivation:`Differing from the embedded model approach, this method compromises simplicity for flexibility.
\u2192 flexible model deployment needed`,solution:`The trained machine learning model becomes a dependency of a separate Machine Learning API service.
A separate microservice has been dedicated to Machine learning and is exclusively responsible for returning the prediction.`,consequences:`pre-trained: yes
on-the-fly-predictions: -

maintain a separate service there is increased complexity with this architecture, but there is more flexibility since the model deployments are now independent of the main application deployments. 

 model microservice or main server can be scaled separately to deal with higher volumes of traffic or to potentially serve other applications`,examples:"",related:"",categories:"Deployment",resources:113},{id:"042",name:"Model Published as Data",aka:"",motivation:"There is a simplicity-flexibility trade-off in deployment; it is required to ingest the model at runtime",solution:"The training process publishes a trained model to a streaming platform (i.e. Apache Kafka) which will be consumed at runtime by the application, instead of build time \u2014 eligible to subscribe for any model updates.",consequences:`Pre-trained: yes,
On-the-fly-predictions: yes. Maintaining the infrastructure required for this archeticutre demands much more engineering sophistication, however ML models can be updated without any applications needing to be redeployed \u2014 this is because the model can be ingested at runtime.`,examples:"",related:"",categories:"Deployment",resources:113},{id:"043",name:"Strategy Pattern",aka:"",motivation:"How can an ML model that performs a task in a given context be flexibly changed?",solution:"Define an interface (strategy) that different models implement. The context will call the methods exposed by the interface, and the implemented models will behave differently based on the contextual data.",consequences:"Switching models or achieving flexibility in model behavior is easier, but code complexity is increased.",examples:"XGBoost's custom objective functions, Hugging Face's pipeline interface",related:"",categories:"Implementation, Traditional",resources:"120, 103, 012126"},{id:"044",name:"State Pattern",aka:"",motivation:"",solution:"Allow an object to alter its behavior when its internal state changes",consequences:"",examples:"",related:"",categories:"Implementation, Traditional",resources:120},{id:"045",name:"Builder Pattern",aka:"",motivation:"",solution:"Separate the construction of a complex object from its representation",consequences:"",examples:"",related:"",categories:"Implementation, Traditional",resources:120},{id:"046",name:"Adapter Pattern",aka:"",motivation:"",solution:"Convert the interface of a class into another interface clients expect.",consequences:"",examples:"",related:"",categories:"Implementation, Traditional",resources:"120, 103"},{id:"047",name:"Decorator Pattern",aka:"",motivation:"",solution:"Attach additional responsibilities to an object dynamically.",consequences:"",examples:"",related:"",categories:"Implementation, Traditional",resources:"120, 103"},{id:"048",name:"Offline Predictions",aka:"",motivation:"More flexibility in model deployment is needed",solution:`Asynchronous approach
Predictions are triggered and run asynchronisously either by the application or as a scheduled job. The predictions will be collected and stored \u2014 this is what the application uses to serve the predictions via a user interface.`,consequences:`Pre-trained: yes,
On-the-fly-predictions: no. Many in industry have moved away from this architecture, but it\u2019s much more forgiving in a sense that predictions can be inspected before being returned to a user. Therefore, we reduce the risk of our ML system making errors since predictions are not on the fly.`,examples:"",related:"",categories:"Deployment",resources:113},{id:"049",name:"Web single pattern",aka:"",motivation:"When you want to quickly release the predictor in the simplest architecture.",solution:"packs all the artifacts and code for the prediction model in a web server",consequences:`Since the single server REST (or GRPC) interface, preprocess, and trained model are in one place, you can create and deploy them as a simple predictor.  Pros:  (1) Ability to use one programming language for the web server, preprocess and inference.
(2) Easy to manage because of it's simplicity. (3) Easier troubleshooting.
(4) Minimal time-to-production post model training.
Cons: (1) Since all components are packed in a server or a docker image, applying a small patch will require updating the whole image (2) Updates will also require a service deployment, which in bigger organizations requires one to follow a hefty SDLC`,examples:"https://github.com/shibuiwilliam/ml-system-in-actions/tree/main/chapter4_serving_patterns/web_single_pattern",related:"",categories:"Deployment",resources:124},{id:"050",name:"Synchronous pattern",aka:"",motivation:"When in your business logic, the model inference is a blocker to proceed to the next step",solution:"blocks the system's workflow until the prediction finishes",consequences:`Easy to manage with its simplicity. All operational aspects like transaction tracing, monitoring etc become easy as well. Service workflow becomes simple for the process won't proceed until the prediction completes.
But: (1) The prediction latency can become a performance bottleneck.
(2) You might have to consider a workaround for not degrading your user experience because of prediction latency. (3) If the client of your service is another service then this pattern leads to blocked threads on the client side.`,examples:"https://github.com/shibuiwilliam/ml-system-in-actions/tree/main/chapter4_serving_patterns/synchronous_pattern",related:"Asynchronous Pattern",categories:"Deployment, Architecture, Traditional",resources:124},{id:"051",name:"Asynchronous pattern",aka:"",motivation:"When in your business logic, the next step in the process does not depend on the prediction.",solution:"Place a queue or cache in between the client and predictor",consequences:"Separation of a prediction request and prediction retrieval.",examples:"https://github.com/shibuiwilliam/ml-system-in-actions/tree/main/chapter4_serving_patterns/asynchronous_pattern",related:"Synchronous Pattern",categories:"Deployment, Architecture, Traditional",resources:124},{id:"052",name:"Microservice vertical pattern",aka:"",motivation:"When you need to run several inferences in order; When you have several inferences, and they have dependencies",solution:"The pattern deploys prediction models in separate servers or containers as service. You execute prediction request from top to bottom synchronously, and gather the results to respond to the client.",consequences:`Pros: You may run multiple predictions in order.
It is possible to select next prediction service depending on the former prediction.
You may make the resource usage efficient, independent in server and code, and isolate failure. Cons: Since the predictions run synchronous order, it will require higher latency.
A former prediction latency may be bottleneck.
Complex system structure and workflow.`,examples:"-",related:"Microservice horizontal pattern",categories:"Architecture",resources:"124, 012032, 012113, 012128"},{id:"053",name:"Microservice horizontal pattern",aka:"",motivation:"When the workflow can execute multiple predictions in parallel. When you want to integrate prediction results at last. Required to run several predictions to one request",solution:"Multiple are deployed models in parallel. You can send one request to the models at once to acquire multiple predictions, or an integrated prediciton.",consequences:`Pros: (1) You can tune resource usage independently and isolate failure. (2) You may develop model and system without having dependency on other models.                                Cons: (1) The system may become complex.
For synchronous usecase, the slowest inference becomes bottleneck. (2) For asynchronous usecase, you have to let the postprocess to manage time lag between prediction latencies.`,examples:"https://github.com/shibuiwilliam/ml-system-in-actions/tree/main/chapter4_serving_patterns/horizontal_microservice_pattern",related:"Microservice vertical pattern",categories:"Architecture",resources:"124, 012032, 012113, 012128"},{id:"054",name:"Prediction cache pattern",aka:"",motivation:`When your service may get prediction request on same data, and can identify it.
The prediction result may not change so frequently.
The input data can be searched with cache.
To accelerate prediction and offload.`,solution:"The prediction server or proxy will store input data as cache key with prediction as value, if the key does not exist. After the cache, the cache search and prediction will be executed parallelly and returns the value if the cache hits without waiting for the prediction completes.",consequences:`It will shorten the amount of time taken to predict with less load to the prediction server.
The amount of data to cache may need to be considered with balance of cost and volume. The unit price for cache space tends to be higher with less size than storage or database, hence it is recommended to plan a policy to clear cache.
If the prediction result changes with time, it requires to clear old cache to prevent responding with outdated prediction. If the service gets high load that the cache size increases rapidly, it is important to plan cache clear policy concretely.`,examples:"https://github.com/shibuiwilliam/ml-system-in-actions/tree/main/chapter4_serving_patterns/prediction_cache_pattern",related:"Data Cache Pattern",categories:"Architecture, Traditional",resources:124},{id:"055",name:"Data cache pattern",aka:"",motivation:"When you have prediction request for same and repeated data. When the data can be identified with cache key. When you want to accelerate data retrieval and processing.",solution:`There are two variants for the pattern. (1) Cache input data. In this case, the service will request input data to DWH and cache parallelly, and proceed to prediction request if cache hits. If not hit, then you will add the data into the cache along with requesting the prediction. You may cache the data after preprocess, to decrease load for preprocessing.
(2) Preprocess or cache before request. In this case, you will cache data prior to the prediction request, possibly along with the service's first data generation. Once you get a prediction request to the data, you will retrieve from cache to send to prediction. It is a good choice if you want to increase cache rate to improve latency.`,consequences:"Pros: To offload request to prediction server and improve performance. Able to respond quickly. Cons: Cache server cost. Needs a policy for clearing the cache.",examples:"https://github.com/shibuiwilliam/ml-system-in-actions/tree/main/chapter4_serving_patterns/data_cache_pattern",related:"Prediction Cache Pattern",categories:"Architecture, Traditional",resources:124},{id:"056",name:"Circuit break pattern",aka:"",motivation:"When the amount or frequency of requests to prediction suddenly increases, it can outpace the velocity of scaling. Complete outages are to avoid",solution:"It is reasonable to choose to cancel a certain number of request to stabilize the current prediction service, until the scale out completes to respond all the predictions.",consequences:`Pros: Avoid complete outage of the service.
It allows to scale out prediction servers without lowering the current resource availability. Cons:Requires a fallback plan for cancelled requests.`,examples:"-",related:"",categories:"Testing & Quality Assurance, Traditional",resources:124},{id:"057",name:"Prep-pred pattern",aka:"",motivation:`When you have different code-base, library or resource load requirements for preprocess and prediction.
When you want to separate preprocess with prediction to divide issue in each resource to improve availability`,solution:"Divide preprocessing steps and the prediction model into separate resources",consequences:`Pros: (1) Able to use resource efficiently with fault isolation.
(2) Flexibility in resource usage and scalability.
(3) Able to select library and language versions for each component.
Cons: (1) Increases operation cost with complexity in resources.
(2) Network connection in between preprocess and prediction may become bottleneck.`,examples:"https://github.com/shibuiwilliam/ml-system-in-actions/tree/main/chapter4_serving_patterns/prep_pred_pattern",related:"",categories:"Deployment",resources:124},{id:"058",name:"Serving template pattern",aka:"",motivation:`When you need to develop and release a tons of prediction serving with same input and output with common interface
When you can standardize components in serving except for a prediction smodel`,solution:"Standardize the codebase, infrastructure and deployment policy. it is possible to commonize infrastructure layer, including monitoring, alert, logging, authorization, authentication and security",consequences:`Pros: Efficiency improvement
Manageable by a common policy. Con: Updating the template requires consideration and backward compatibility`,examples:"https://github.com/shibuiwilliam/ml-system-in-actions/tree/main/chapter4_serving_patterns/template_pattern",related:"Infrastructure as Code",categories:"Deployment, Traditional",resources:124},{id:"059",name:"Model-in-image pattern",aka:"",motivation:"When you want to unify versions of service environment and prediction model.",solution:"Including the model file into the server or container image. In the model-in-image pattern, you build a server or container image with the model file contained in it.",consequences:`Pros: Uniquely identify a server image with model file version.
Cons: You need to define a complete pipeline of a model training and image building.
Takes longer to build image and deploy.`,examples:"https://github.com/shibuiwilliam/ml-system-in-actions/tree/main/chapter3_release_patterns/model_in_image_pattern",related:"",categories:"Deployment",resources:124},{id:"060",name:"Parameter-based serving pattern",aka:"",motivation:`To control prediction with parameter.
When it is possible to control rule-based.`,solution:"Add a mechanism in your code to change behaviour of all or a part of the predictions, including stopping, retrying or timeout. Since it is difficult to keep high accuracy without any error for a model for every case or every input data, it is better to estimate possible anomaly and control with rulebased mechanism.",consequences:`Pros: avoid abnormal prediction for edge cases.
Cons: Impossible to cover all the cases.
Increase in rules make complexity in operation.`,examples:"https://github.com/shibuiwilliam/ml-system-in-actions/tree/main/chapter6_operation_management/paramater_based_pattern",related:"",categories:"Implementation",resources:124},{id:"061",name:"Model-load pattern",aka:"",motivation:`When update cycle of a model is more frequent than server image update.
When you want to reuse a server image for serving multiple models.`,solution:"Build and save the prediction server image and model file separately, making the image light-weight with respect to size.",consequences:`Pros: Separate model versioning and image versioning. Reuse of server image. Light image size.
Change management of server image becomes easier. Cons: It may take longer to start the service. The server although up should be treated as unhealthy until the model load completes.
A new requirement of matching supported library versions between images and models is applicable for this pattern.`,examples:"https://github.com/shibuiwilliam/ml-system-in-actions/tree/main/chapter3_release_patterns/model_load_pattern#",related:"",categories:"Deployment",resources:124},{id:"062",name:"Data Warehouse for ML",aka:"",motivation:`A data warehouse is required for mining/extracting new knowledge in the form of a database. A copy of an operational database may impede Data Mining Algorithms due to their structure. 

One want to keep historic data (cleaned and uncleaned)`,solution:`Specific strucutre of a Data Warehouse facilitates Data Mining Algorithms.

Example: claimed data can be modified and corrected in the system. At the same time a Machine Learning algorithm can use this data to train.`,consequences:"",examples:"",related:"",categories:"Architecture",resources:"012013"},{id:"063",name:"Closed Loop intelligence",aka:"",motivation:"ML System performs bad on the day of shipping",solution:"Close loop between user and ML-system. Acquire data to improve system",consequences:"",examples:"012026",related:"",categories:"Implementation",resources:"012026"},{id:"064",name:"Daisy Architecture",aka:"",motivation:"Example: organisations acquire the ability to scale their premium content production processes via the use of machine learning, text analytics and annotation techniques, and then extend the coverage of that tooling over as much of their remaining content geography as possible",solution:"Switch from content production pipelines that are a.) push-based; b.) manual and c.) linear; to content enrichment cycles that are a.) pull-based; b.) automated; and c.) on-demand and iterative",consequences:`Result of three technical enablers:
(1) Influence of Kanban on software design
(2) Scaling of automated content analysis and rich metadata creation
(3) Adoption of microservice architectures`,examples:"",related:"",categories:"Architecture",resources:"005008, 001013, 012029"},{id:"065",name:"Kappa Architecture Pattern",aka:"",motivation:"Deal with large amount of date with less code resources",solution:"Use one processing engine for both stream and batch processing",consequences:"",examples:"",related:"",categories:"Architecture, Traditional",resources:"001013, 012113, 012128"},{id:"066",name:"Data Lake (for ML)",aka:"",motivation:`Collect raw data for as long as possible and offer access to analytic algorithms, like ML algorithms.


In the traditional Data Warehouse architecture, all data is read from various sources by an ETL (Extract/Transform/Load) tool, and is well structured based on a simplified data model to gain all the advantages of a data warehouse such
as fast and efficient query processing [Daehn 2017]. In other words, semantics first and content later [Daehn 2017].
In practice, neither the types of analyses that will be performed nor the adopted frameworks can be foreseen.`,solution:'Data, which ranges from structured to unstructured, should be stored as \u201Craw\u201D as possible into a data storage called a "Data Lake". A data lake should allow parallel analyses of different types of data with various frameworks. Thus, a data lake should facilitate efficient (time, space) writing of data from the data sources (e.g., sensors) and efficient, parallel reading of the data for ML frameworks. To realize these features, the data lake must contain historical data and support the insert functionality.',consequences:"",examples:"",related:"Distinguish Business Logic from ML Models, Gateway Routing Architecture",categories:"Architecture",resources:"001013, 012113, 012128"},{id:"067",name:"Discard PoC code",aka:"",motivation:"Code written for a PoC often includes shortcuts, is hard to maintain or contains unused code lines",solution:"Discard PoC code and rebuild the code using the findings of the PoC code",consequences:"",examples:"",related:"",categories:"Process, Traditional",resources:"012113, 012128"},{id:"068",name:"Parameter-Server Abstraction",aka:"",motivation:"Abstractions for distribute learning are lacking",solution:"Distribute data and workload over worker nodes. Server nodes has globally shared parameters.",consequences:"",examples:"",related:"",categories:"Architecture",resources:"012113, 012128"},{id:"069",name:"Data flows up, Model flows down",aka:"",motivation:"",solution:"Use a shared model to predict in the cloud + use federated learning approach on the client. Pos: keep all training data on device",consequences:"",examples:"",related:"",categories:"Implementation",resources:"012113, 012128"},{id:"070",name:"Condition-based-serving pattern",aka:"",motivation:"When prediction target varies a lot by condition. When you can conditionally select a model in a rule-based manner.",solution:"Deploy various models and define selection rules for them. Depending on certain attributes of the input data, the most adequate model is used.",consequences:`Positive: Select an adequate model depending on condition.
Negative: Operational cost raise with number of models.`,examples:"https://github.com/shibuiwilliam/ml-system-in-actions/tree/main/chapter6_operation_management/condition_based_pattern",related:"",categories:"Deployment",resources:124},{id:"",name:"",aka:"",motivation:"",solution:"",consequences:"",examples:"",related:"",categories:"",resources:""},{id:"",name:"",aka:"",motivation:"",solution:"",consequences:"",examples:"",related:"",categories:"",resources:""},{id:"",name:"",aka:"",motivation:"",solution:"",consequences:"",examples:"",related:"",categories:"",resources:""},{id:"",name:"",aka:"",motivation:"",solution:"",consequences:"",examples:"",related:"",categories:"",resources:""}];export{e as default};
