const e="055",t="Data cache pattern",a="",o="When you have prediction request for same and repeated data. When the data can be identified with cache key. When you want to accelerate data retrieval and processing.",c=`There are two variants for the pattern. (1) Cache input data. In this case, the service will request input data to DWH and cache parallelly, and proceed to prediction request if cache hits. If not hit, then you will add the data into the cache along with requesting the prediction. You may cache the data after preprocess, to decrease load for preprocessing.
(2) Preprocess or cache before request. In this case, you will cache data prior to the prediction request, possibly along with the service's first data generation. Once you get a prediction request to the data, you will retrieve from cache to send to prediction. It is a good choice if you want to increase cache rate to improve latency.`,r="Pros: To offload request to prediction server and improve performance. Able to respond quickly. Cons: Cache server cost. Needs a policy for clearing the cache.",i="https://github.com/shibuiwilliam/ml-system-in-actions/tree/main/chapter4_serving_patterns/data_cache_pattern",n=["Prediction Cache Pattern"],s=["Architecture","Traditional"],h=["124"],d={id:e,name:t,aka:a,motivation:o,solution:c,consequences:r,examples:i,related:n,categories:s,resources:h};export{a as aka,s as categories,r as consequences,d as default,i as examples,e as id,o as motivation,t as name,n as related,h as resources,c as solution};
