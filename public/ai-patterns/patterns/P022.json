{
	"id": "022",
	"name": "Batch Serving",
	"aka": "",
	"motivation": "Predictions need to be carried out asynchronously over large volumes of data (contrary to predictions for small, individual requests), e.g., generating personalized playlists every week. This is only applicable if there is no need for (near-)real-time predictions.",
	"solution": "Use distributed data processing infrastructure (e.g, based on MapReduce) to asynchronously carry out complex ML inference tasks on a large number of computing nodes. The individual predictions are aggregated back into a single result.",
	"consequences": "Positive: You can manage server resources flexibly and in strict accordance with demand. You may rerun the job in case of error. There is no requirement for high availability in your server system. Negative: You need a job management server. This pattern depends on the ability to split a task across multiple workers.",
	"examples": "",
	"related": [
		"Stateless Serving Function"
	],
	"categories": [
		"Deployment",
		"Traditional"
	],
	"resources": [
		"101",
		"106",
		"124"
	]
}