{
	"id": "042",
	"name": "Model Published as Data",
	"aka": "",
	"motivation": "There is a simplicity-flexibility trade-off in deployment; it is required to ingest the model at runtime",
	"solution": "The training process publishes a trained model to a streaming platform (i.e. Apache Kafka) which will be consumed at runtime by the application, instead of build time — eligible to subscribe for any model updates.",
	"consequences": "Pre-trained: yes,\nOn-the-fly-predictions: yes. Maintaining the infrastructure required for this archeticutre demands much more engineering sophistication, however ML models can be updated without any applications needing to be redeployed — this is because the model can be ingested at runtime.",
	"examples": "",
	"related": [
		""
	],
	"categories": [
		"Deployment"
	],
	"resources": [
		"113"
	]
}